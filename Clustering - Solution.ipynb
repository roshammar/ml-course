{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "For this lab, we are going to do **unsupervised** learning on **unstructured** data!\n",
    "\n",
    "More specificially, we will be doing **clustering** on news articles.\n",
    "\n",
    "Along the way, we will also learn a simple way to **summarize** texts with a few representative words.\n",
    "\n",
    "It is unsupervised learning because we do not provide the algorithms with any labeled data. There are no correct answers, just raw, unlabeled texts. We want the algorithm to find patterns in the data and divide the different texts into groups, *clusters*, such that articles that are in the same cluster are similar in some aspect. It is up to us to interpret what aspect that might be.\n",
    "\n",
    "It is unstructured data, because each news article is just a long string of words. There are no numbers in a neat columnar structure and the articles are of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "The data consists of 10 000 Swedish news articles, crawled from an online news site, and are a sample from news articles published during two years.\n",
    "\n",
    "Our hope is that we will recognize both general (robberies, movie reviews) and specific (the Svenska Akademien affair) clusters of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Since we are dealing with unstructured data, chances are that we will *not* load it from a csv file, since these are all about structure.\n",
    "\n",
    "This time, the data is in **JSON** format, the same format that many document (or NoSQL) databases use.\n",
    "The format has its origin from JavaScript data types, but is also very close to Python's data types, in particular we will recognize dicts {} and lists [].\n",
    "\n",
    "Actually, I lied. It is not really in JSON format, but a variant of it that I think is somewhat underused.\n",
    "\n",
    "There is a big problem with JSON and somewhat large datasets. Imagine you have a file containing a list of ten million items. Even if you were only interested in the ten first items, the JSON decoder would still need to read all ten million items into memory, just to correctly parse the list. That is kind of harsh on the memory usage.\n",
    "\n",
    "Fortunately, there is a simple solution.\n",
    "\n",
    "In the **JSONL** format, each line is individually encoded as a JSON object. You can then read as few lines as you wish from the file, and don't load anything more than necessary into memory. If you *do* want to use all objects in the file, you can do so in a streaming manner, and possibly use little memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to import `requests`, a library that makes downloading things easy, and `json` to parse the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/slides.tenfifty.io/arts_sample.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the given `URL`, but keep the data in memory. No need to save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as Python sees it, this is just one really long string.\n",
    "We need to parse it. Use the `json` module to decode this string into a Python list of strings (the individual articles).\n",
    "\n",
    "Let the variable `texts` refer to this decoded list.\n",
    "\n",
    "Do you get the error `JSONDecodeError: Extra data: line 2 column 1`?\n",
    "Remember that this is JSON**L**, not plain JSON. We will need to split the long string into individual lines and parse each line as JSON individually. A *list comprehension* would be super handy for this, but you could also append line by line to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [json.loads(line) for line in r.text.splitlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many articles are there? There should be 10 000.\n",
    "\n",
    "Print the first one. You should always take a quick look at the data to see what you are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Väpnat rån i Hyllie\n",
      "Strax innan 21 på torsdagskvällen blev en butik vid Citytunnelns Hylliestation utsatt för ett rån. De två maskerade gärningsmännen ska, enligt polisen, ha varit beväpnade med en pistol. Rånarna lämnade sedan platsen springande. Inga personer uppges ha kommit till skada och det är ännu okänt om rånarna fått med sig något byte.\n"
     ]
    }
   ],
   "source": [
    "print(len(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we are ready to go!\n",
    "\n",
    "The problem is, we now have a bunch of strings, but typically ML algorithms work with numbers. We need a way to convert the texts.\n",
    "\n",
    "Also, since our two tasks are to find a way to summarize the articles, and to cluster them by contents (topics), it would be nice if we could weight the words, so that common words that appear in almost all articles nostly will be ignored.\n",
    "\n",
    "Fortunately, the `sklearn` library has our back.\n",
    "\n",
    "It contains a class TfidfVectorizer that does exactly this.\n",
    "\n",
    "And *this* is quite a lot! More specifically, it:\n",
    "\n",
    "1. Splits each text up into individual words, ignoring punctuation, etc.\n",
    "1. Makes a dictionary that maps words to unique numerical IDs\n",
    "1. Throws away words that occur less that some limit, to get rid of spelling errors and uncommon words\n",
    "1. Trows away words that are too common and thus are not helpful\n",
    "1. Calculates a weight for each word in each document, such that a high weight means that the word is important. These are words that occur frequently in the current document, but has a low frequency overall in all documents.\n",
    "1. Converts each document to a row in a matrix, where each column represents a specific word, and the value is the count of the word in this document * the calculated weight of the word\n",
    "1. Actually makes this into a sparse matrix, since it would be huge otherwise\n",
    "\n",
    "Not bad at all!\n",
    "\n",
    "We start by importing the TfidfVectorizer and creating an instance of it with some suitable settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.3, min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must **fit** (let the Vectorizer gather its word count statistics that it needs to compute the word weights) and **transform** (actually do this transformation on our texts) the vectorizer.\n",
    "\n",
    "This can be done in one step. Check out the documentation.\n",
    "\n",
    "Save the result from this operation as `X`, since it is our transformed training data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now a matrix. What dimensions do you expect it to have?\n",
    "You should know the number of rows, and perhaps you have a rough estimate for the number of columns?\n",
    "Print the dimensions, or the *shape* of the matrix to see if you were right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 53276)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we wanted a way to summarize a document with a few keywords?\n",
    "\n",
    "Well, the vectorizer makes this quite easy.\n",
    "\n",
    "It has already the weights for all words, so if we just *transform* a document, it will replace the words in the document with their weights multiplied by their counts -- in short, a way of assigning importance to words. So if we just can find the highest numbers after this transformation, map them back to their word values, and sort this list, we might have something useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, transform the first text using our vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vectorizer.transform([texts[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to find the score and actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "weights_sk = [(feature_names[col], result[0, col]) for col in result.nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, weights_sk is a list of tuples, where each tuple is of the form (word, score).\n",
    "\n",
    "Can you sort this list so that the words with the highest scores come first, and print the first 10 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rånarna, rån, väpnat, hyllie, springande, byte, maskerade, torsdagskvällen, okänt, beväpnade'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join([word for word, weight in sorted(weights_sk, key=lambda i: i[1], reverse=True)[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the article again for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Väpnat rån i Hyllie\n",
      "Strax innan 21 på torsdagskvällen blev en butik vid Citytunnelns Hylliestation utsatt för ett rån. De två maskerade gärningsmännen ska, enligt polisen, ha varit beväpnade med en pistol. Rånarna lämnade sedan platsen springande. Inga personer uppges ha kommit till skada och det är ännu okänt om rånarna fått med sig något byte.\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad summary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets see if we can find some clusters of articles that share the same topic.\n",
    "\n",
    "We will use a clustering algorithm called `KMeans` (not to be confused with KNN (k-Nearest Neighbors), which is a supervised learning algorithm used for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this algorithm, we will have to specify how many clusters we expect, and also for how many iterations it should work. There are other clustering algorithms that can figure this out on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 100\n",
    "km = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have observed, you call `fit` when you want an algorithm in `sklearn` to learn your data.\n",
    "\n",
    "Make the clusterer learn our training data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=100, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we iterate through each cluster and print some representative words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: du grönsaker frukt grönt din igen äta många olika känner\n",
      "Cluster 1: sas vattenfall kronor företaget kunder bolaget 000 strejken 100 konsumentverket\n",
      "Cluster 2: vädret sommaren sommar blir sol vecka augusti semestern sverige solen\n",
      "Cluster 3: polisen tomten platser personer många varnar människor folk fått inbrott\n",
      "Cluster 4: pojken pojke pojkens son honom mamma mamman polisen hans hade\n",
      "Cluster 5: löfven stefan lööf kristersson partiledare annie alliansen socialdemokraterna björklund vill\n",
      "Cluster 6: rättegången åklagaren åtalas åtalade åringen fängelse grovt brott mannen mot\n",
      "Cluster 7: procent undersökning undersökningen sverige visar än länder många sveriges svenskarna\n",
      "Cluster 8: polisen mannen hade fick bilen larmade honom lyckades full polis\n",
      "Cluster 9: mobilen mobil du skärm iphone samsung android galaxy sony skärmen\n",
      "Cluster 10: inbrott polisen tjuvarna tjuven du polisens varnar din tjuv bostadsinbrott\n",
      "Cluster 11: djur katter hundar katt katten hundarna djuren hund länsstyrelsen katterna\n",
      "Cluster 12: new york post hon yorks skriver the dollar times usa\n",
      "Cluster 13: skolan skolans rektorn barn föräldrar vill elever skola mobbning ta\n",
      "Cluster 14: skriver mot vill the nya alla få sverige sina går\n",
      "Cluster 15: hon mig hade skulle min fick berättar blev hennes någon\n",
      "Cluster 16: henne hennes johansson marina hon polisen åringen sambo mord huset\n",
      "Cluster 17: anmälningar antalet anmälningarna ökat fall senaste radio mot fått procent\n",
      "Cluster 18: ryssland ryska putin ukraina vladimir krim rysk moskva rysslands president\n",
      "Cluster 19: misstänkt polisen misstänks skogen hittades wesström två mordet mannen hon\n",
      "Cluster 20: hon fick hennes hade gravid magen läkarna barn sjukhuset mamma\n",
      "Cluster 21: sd sverigedemokraterna partiet sverigedemokraternas ekeroth partiets kent riksdagen expressen politiker\n",
      "Cluster 22: ask brevet beatrice brev svt justitieminister skriver jakten hur någon\n",
      "Cluster 23: bilder facebook buzzfeed bilderna hon hennes sajten bild bilden amerikanska\n",
      "Cluster 24: hade bilen youtube fick plötsligt tog blev fånga kunde klippet\n",
      "Cluster 25: hon mig min blev hade hennes mitt fick själv liv\n",
      "Cluster 26: pappan flickan dottern åtalas hon olaga dotter misshandel grov henne\n",
      "Cluster 27: mannen fängelse hans åringen hade årig honom polisen hon årige\n",
      "Cluster 28: sömn sova du studie sover somna studenter studien minuter visar\n",
      "Cluster 29: bilar bilen bil kör köra trafikverket du många olyckor trafiken\n",
      "Cluster 30: skola skolinspektionen skolan barn elever lundsberg barnen föräldrar lundsbergs skriver\n",
      "Cluster 31: cannabis marijuana narkotika lagligt kilo narkotikabrott droger knark drogen polisen\n",
      "Cluster 32: hon facebook mig hur mot skriver bild metro fått du\n",
      "Cluster 33: du dig gör din hur dina se vatten bara blir\n",
      "Cluster 34: forskarna forskare 3d materialet användas independent ström du ögat gör\n",
      "Cluster 35: prinsessan madeleine chris neill prins estelle slottskyrkan carl philip slottet\n",
      "Cluster 36: räddningstjänsten bilar bränderna bränder platsen polisen frölunda göteborg gt personer\n",
      "Cluster 37: kvadratmeter kronor lägenheter dyraste miljoner 000 lägenheten lägenheterna stockholm lägenhet\n",
      "Cluster 38: procent priserna bostadsrätter bostäder stockholm bostadsmarknaden mäklare marknaden lägenheter vd\n",
      "Cluster 39: procent socialdemokraterna moderaterna yougov väljare parti sd sverigedemokraterna valet mätning\n",
      "Cluster 40: du dig vill vad bra hur många gör få blir\n",
      "Cluster 41: öl alkohol liter dricka iq dricker tullen vin sprit alkoholen\n",
      "Cluster 42: tv serien netflix serier the tittare se serie spelar hbo\n",
      "Cluster 43: miljarder kronor världens dollar procent rikaste miljoner 000 året förmögenhet\n",
      "Cluster 44: polisen död hittades mord hittats misstänkt lägenhet lägenheten misstänker stockholm\n",
      "Cluster 45: döms mannen fängelse kvinnan åringen tingsrätt hon tingsrätten våldtäkt skadestånd\n",
      "Cluster 46: misstänkt mannen sannolika mord skäl årig åringen årige häktad polisen\n",
      "Cluster 47: sexuella övergrepp trakasserier kvinnor metoo sexuellt mot män utsatts hon\n",
      "Cluster 48: kronor försäkringskassan 000 åtalas hon kvinnan vab ersättning barn föräldrapenning\n",
      "Cluster 49: domen bergwall sture hovrätten quick dömdes tingsrätten fängelse dom mord\n",
      "Cluster 50: fartyget fartyg larm larmet rom italien skeppet personer kustbevakningen 300\n",
      "Cluster 51: hon hans fick hade hennes blev henne news sitt skulle\n",
      "Cluster 52: hunden hund hundar polisen hon tog hade bil bilen djurplågeri\n",
      "Cluster 53: appen du kl 00 dig 18 stockholm augusti 30 pride\n",
      "Cluster 54: apple iphone apples ipad nya samsung mobilen nästa ios teknikjätten\n",
      "Cluster 55: studien studie visar forskarna forskare ny än forskning risken hur\n",
      "Cluster 56: ivo inspektionen omsorg vård anmälan lex sjukhuset kvinnan händelsen läkaren\n",
      "Cluster 57: persson schibbye etiopien martin journalisterna johan svenskarna svenska fängslade ud\n",
      "Cluster 58: procent jämfört året än ökat försäljningen ökar priserna sverige scb\n",
      "Cluster 59: barn barnen hon få vill pengar göra kronor alla många\n",
      "Cluster 60: kronor systembolaget 000 lurade lurat pengar miljoner trolöshet huvudman förskingring\n",
      "Cluster 61: polisen misstänkta misstänkt mannen misstänks männen två efterlyst grov mot\n",
      "Cluster 62: is islamiska syrien staten irak anonymous mot attacken terrorgruppen theft\n",
      "Cluster 63: reinfeldt juholt mona sahlin håkan statsminister fredrik jämtin regeringen carin\n",
      "Cluster 64: flickan flicka polisen hon årig henne missing people flickans mannen\n",
      "Cluster 65: sl förseningar resenärer trafiken stockholm tåg tågen tågtrafiken tunnelbanan sj\n",
      "Cluster 66: kronor 000 skatteverket betala miljoner avgift betalar per skatt du\n",
      "Cluster 67: planet flygplan passagerare flygplanet ombord flygbolaget piloten flygplatsen passagerarna plan\n",
      "Cluster 68: nya miljarder bygga bostäder 000 stockholms kronor stockholm spårväg hagastaden\n",
      "Cluster 69: åkesson jimmie sd sverigedemokraterna sverigedemokraternas åkessons partiledare ledaren svt partiledaren\n",
      "Cluster 70: bussen buss hon skulle fick hade föraren chauffören blev körde\n",
      "Cluster 71: hon sjukdomen dag hur mycket du få barn många vad\n",
      "Cluster 72: personalen polisen svt mannen eskilstuna chefen fick hade blev badhuset\n",
      "Cluster 73: google nsa googles användare information snowden usa facebook maps amerikanska\n",
      "Cluster 74: forskare forskarna jorden miljoner arter plast tror jordens människan wwf\n",
      "Cluster 75: personer skadade polisen två uppger minst skadats afp skottlossning döda\n",
      "Cluster 76: grottan pojkarna thailändska instängda tränare fotbollslaget grotta luang ur pojkar\n",
      "Cluster 77: boston säpo polisen hot säkerhetspolisen mot eurovision sverige två explosionerna\n",
      "Cluster 78: mot al muslimska personer kairo brödraskapet människor dog world världen\n",
      "Cluster 79: elever skolan lärare eleverna skolor elev läraren skola lärarna skolverket\n",
      "Cluster 80: viruset sjukdomen du drabbas folkhälsomyndigheten vaccin barn virus fall många\n",
      "Cluster 81: löfven eu stefan fn statsminister sverige syrien usa president brexit\n",
      "Cluster 82: julen jul du vill julafton många gör alla ohälsa fira\n",
      "Cluster 83: grader smhi meteorolog väder regn helgen södra svealand vädret temperaturer\n",
      "Cluster 84: polisen sverige migrationsverket personer mot antalet brott fall polisens många\n",
      "Cluster 85: sms svt du varnar klicka länken länk mejl et skriver\n",
      "Cluster 86: facebook du viralgranskaren bilden sociala medier nej bild inlägget användare\n",
      "Cluster 87: du akademien svenska akademiens peter vad hur årets ja hon\n",
      "Cluster 88: nordkorea jong un kim nordkoreas trump usa nordkoreanska kärnvapen donald\n",
      "Cluster 89: jorden rymden nasa planeten mars planet rymdstationen planeter månen forskare\n",
      "Cluster 90: filmen trailern se film the filmerna du hans sagan jackson\n",
      "Cluster 91: quiz du testa hur vad vet metros koll kunskaper egentligen\n",
      "Cluster 92: romer diskriminering romska diana nyman hon personalen äta nekades diskrimineringsombudsmannen\n",
      "Cluster 93: branden brand räddningstjänsten brinna mordbrand polisen elden släcka brann eld\n",
      "Cluster 94: kvinnan kvinna hon mannen polisen henne hennes kvinnans hade åriga\n",
      "Cluster 95: trump obama president donald usa barack presidenten obamas trumps romney\n",
      "Cluster 96: of game thrones the to and is that this swedish\n",
      "Cluster 97: polisen platsen sjukhus mannen misstänkt fördes strax klockan polis kom\n",
      "Cluster 98: breivik oslo behring norska anders breiviks norge fängelset utøya massakern\n",
      "Cluster 99: window js async script infogram http https if process createelement\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % feature_names[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the topics make sense to you?\n",
    "\n",
    "Pretty impressive, huh, given that the algorithm does not know anything about what the words actually mean!\n",
    "\n",
    "This is an example of how we still can make sense of data, even if it is unstructured (text) and we have no labels (unsupervised learning).\n",
    "\n",
    "We could make this even better by giving the preprocessing of the texts some more love. We could for example transform each word into its base form (eleverna -> elev, mamman -> mamma) and remove common words that still make it through the weighting process.\n",
    "\n",
    "We could also try a hierarchical clustering, or perform a dimensionality reduction and plot the clusters and articles in a two dimensional plot, with similar articles closer to each other.\n",
    "\n",
    "Nevertheless, we reached some pretty good results with not too many lines of code.\n",
    "\n",
    "I hope this has been inspiring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
