{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "For this lab, we are going to do **unsupervised** learning on **unstructured** data!\n",
    "\n",
    "More specificially, we will be doing **clustering** on news articles.\n",
    "\n",
    "Along the way, we will also learn a simple way to **summarize** texts with a few representative words.\n",
    "\n",
    "It is unsupervised learning because we do not provide the algorithms with any labeled data. There are no correct answers, just raw, unlabeled texts. We want the algorithm to find patterns in the data and divide the different texts into groups, *clusters*, such that articles that are in the same cluster are similar in some aspect. It is up to us to interpret what aspect that might be.\n",
    "\n",
    "It is unstructured data, because each news article is just a long string of words. There are no numbers in a neat columnar structure and the articles are of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "The data consists of 10 000 Swedish news articles, crawled from an online news site, and are a sample from news articles published during two years.\n",
    "\n",
    "Our hope is that we will recognize both general (robberies, movie reviews) and specific (the Svenska Akademien affair) clusters of articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Since we are dealing with unstructured data, chances are that we will *not* load it from a csv file, since these are all about structure.\n",
    "\n",
    "This time, the data is in **JSON** format, the same format that many document (or NoSQL) databases use.\n",
    "The format has its origin from JavaScript data types, but is also very close to Python's data types, in particular we will recognize dicts {} and lists [].\n",
    "\n",
    "Actually, I lied. It is not really in JSON format, but a variant of it that I think is somewhat underused.\n",
    "\n",
    "There is a big problem with JSON and somewhat large datasets. Imagine you have a file containing a list of ten million items. Even if you were only interested in the ten first items, the JSON decoder would still need to read all ten million items into memory, just to correctly parse the list. That is kind of harsh on the memory usage.\n",
    "\n",
    "Fortunately, there is a simple solution.\n",
    "\n",
    "In the **JSONL** format, each line is individually encoded as a JSON object. You can then read as few lines as you wish from the file, and don't load anything more than necessary into memory. If you *do* want to use all objects in the file, you can do so in a streaming manner, and possibly use little memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to import `requests`, a library that makes downloading things easy, and `json` to parse the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/slides.tenfifty.io/arts_sample.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the given `URL`, but keep the data in memory. No need to save to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as Python sees it, this is just one really long string.\n",
    "We need to parse it. Use the `json` module to decode this string into a Python list of strings (the individual articles).\n",
    "\n",
    "Let the variable `texts` refer to this decoded list.\n",
    "\n",
    "Do you get the error `JSONDecodeError: Extra data: line 2 column 1`?\n",
    "Remember that this is JSON**L**, not plain JSON. We will need to split the long string into individual lines and parse each line as JSON individually. A *list comprehension* would be super handy for this, but you could also append line by line to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many articles are there? There should be 10 000.\n",
    "\n",
    "Print the first one. You should always take a quick look at the data to see what you are dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we are ready to go!\n",
    "\n",
    "The problem is, we now have a bunch of strings, but typically ML algorithms work with numbers. We need a way to convert the texts.\n",
    "\n",
    "Also, since our two tasks are to find a way to summarize the articles, and to cluster them by contents (topics), it would be nice if we could weight the words, so that common words that appear in almost all articles nostly will be ignored.\n",
    "\n",
    "Fortunately, the `sklearn` library has our back.\n",
    "\n",
    "It contains a class TfidfVectorizer that does exactly this.\n",
    "\n",
    "And *this* is quite a lot! More specifically, it:\n",
    "\n",
    "1. Splits each text up into individual words, ignoring punctuation, etc.\n",
    "1. Makes a dictionary that maps words to unique numerical IDs\n",
    "1. Throws away words that occur less that some limit, to get rid of spelling errors and uncommon words\n",
    "1. Trows away words that are too common and thus are not helpful\n",
    "1. Calculates a weight for each word in each document, such that a high weight means that the word is important. These are words that occur frequently in the current document, but has a low frequency overall in all documents.\n",
    "1. Converts each document to a row in a matrix, where each column represents a specific word, and the value is the count of the word in this document * the calculated weight of the word\n",
    "1. Actually makes this into a sparse matrix, since it would be huge otherwise\n",
    "\n",
    "Not bad at all!\n",
    "\n",
    "We start by importing the TfidfVectorizer and creating an instance of it with some suitable settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.3, min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we must **fit** (let the Vectorizer gather its word count statistics that it needs to compute the word weights) and **transform** (actually do this transformation on our texts) the vectorizer.\n",
    "\n",
    "This can be done in one step. Check out the documentation.\n",
    "\n",
    "Save the result from this operation as `X`, since it is our transformed training data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X is now a matrix. What dimensions do you expect it to have?\n",
    "You should know the number of rows, and perhaps you have a rough estimate for the number of columns?\n",
    "Print the dimensions, or the *shape* of the matrix to see if you were right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we wanted a way to summarize a document with a few keywords?\n",
    "\n",
    "Well, the vectorizer makes this quite easy.\n",
    "\n",
    "It has already the weights for all words, so if we just *transform* a document, it will replace the words in the document with their weights multiplied by their counts -- in short, a way of assigning importance to words. So if we just can find the highest numbers after this transformation, map them back to their word values, and sort this list, we might have something useful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, transform the first text using our vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to find the score and actual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "weights_sk = [(feature_names[col], result[0, col]) for col in result.nonzero()[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, weights_sk is a list of tuples, where each tuple is of the form (word, score).\n",
    "\n",
    "Can you sort this list so that the words with the highest scores come first, and print the first 10 words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the article again for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad summary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets see if we can find some clusters of articles that share the same topic.\n",
    "\n",
    "We will use a clustering algorithm called `KMeans` (not to be confused with KNN (k-Nearest Neighbors), which is a supervised learning algorithm used for both classification and regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this algorithm, we will have to specify how many clusters we expect, and also for how many iterations it should work. There are other clustering algorithms that can figure this out on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 100\n",
    "km = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have observed, you call `fit` when you want an algorithm in `sklearn` to learn your data.\n",
    "\n",
    "Make the clusterer learn our training data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we iterate through each cluster and print some representative words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % feature_names[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the topics make sense to you?\n",
    "\n",
    "Pretty impressive, huh, given that the algorithm does not know anything about what the words actually mean!\n",
    "\n",
    "This is an example of how we still can make sense of data, even if it is unstructured (text) and we have no labels (unsupervised learning).\n",
    "\n",
    "We could make this even better by giving the preprocessing of the texts some more love. We could for example transform each word into its base form (eleverna -> elev, mamman -> mamma) and remove common words that still make it through the weighting process.\n",
    "\n",
    "We could also try a hierarchical clustering, or perform a dimensionality reduction and plot the clusters and articles in a two dimensionsal plot, with similar articles closer to each other.\n",
    "\n",
    "Nevertheless, we reaches some pretty good results with not too many lines of code.\n",
    "\n",
    "I hope this has been inpiring!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
